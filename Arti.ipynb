{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# global varuables\n",
    "w = h = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Reads band images and index images\n",
    "#\n",
    "def read_img(site, _s):\n",
    "    data = []\n",
    "\n",
    "    for i in range(1, 8):\n",
    "        if _s == 'B':\n",
    "            filename = \"input/\"+ site +\"/B\" + str(i) + \"_250_B.tif\"\n",
    "        else:\n",
    "            filename = \"input/\"+ site +\"/B\" + str(i) + \"_250.tif\"\n",
    "            \n",
    "        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "        data.append(img)\n",
    "\n",
    "    index = [\"NDVI.tif\", \"NDWI.tif\", \"NDBI.tif\"]\n",
    "    for i in index:\n",
    "        filename = \"input/\"+ site +\"/\" + i\n",
    "        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "        data.append(img)\n",
    "\n",
    "#     cv2.imshow(\"Image\", data[9])\n",
    "#     cv2.waitKey(0)\n",
    "    \n",
    "    return data\n",
    "\n",
    "i_img = read_img(\"Landsat8-A\", 'A')\n",
    "test_img = read_img(\"Landsat8-B\", 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "def crop_train(_img, _c, site):\n",
    "    _class = \"\"\n",
    "    if(_c == 0):\n",
    "        _class = 'Vegetation'\n",
    "    elif(_c == 1):\n",
    "        _class = 'Built-up'\n",
    "    else:\n",
    "        _class = 'Water'\n",
    "        \n",
    "    _pts = pd.read_excel('input/'+ site +'.xlsx', sheet_name = _class)\n",
    "\n",
    "    # iterate over rows with iterrows()\n",
    "    for index, row in _pts.iterrows():\n",
    "        x = row['X'] - 4\n",
    "        y = row['Y'] - 4\n",
    "        \n",
    "        for img in _img:        \n",
    "            crop_img = img[y:y+h, x:x+w]\n",
    "            med = np.median(crop_img) # get median of 9x9 image\n",
    "            X_tr_list.append(med)\n",
    "        y_tr.append(_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "def crop_test(_img):\n",
    "    _pts = pd.read_excel('input/site-b.xlsx')\n",
    "\n",
    "    for index, row in _pts.iterrows():\n",
    "        x = row['X'] - 4\n",
    "        y = row['Y'] - 4\n",
    "        \n",
    "        if(row['Class'] == 255):\n",
    "            _c = 0\n",
    "        elif(row['Class'] == 128):\n",
    "            _c = 1\n",
    "        else:\n",
    "            _c = 2\n",
    "        \n",
    "        for img in _img:        \n",
    "            crop_img = img[y:y+h, x:x+w]\n",
    "            med = np.median(crop_img) # get median of 9x9 image\n",
    "            X_te_list.append(med)\n",
    "        y_te.append(_c)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "2000\n",
      "200\n",
      "Testing...\n",
      "2000\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# TRAINING SET SITE A ------------------------------------------------------------------\n",
    "X_tr_list = []\n",
    "y_tr = []\n",
    "\n",
    "crop_train(i_img, 0, 'site-a')\n",
    "crop_train(i_img, 1, 'site-a')\n",
    "crop_train(i_img, 2, 'site-a')\n",
    "\n",
    "print(\"Training...\")\n",
    "print(len(X_tr_list))\n",
    "print(len(y_tr))\n",
    "\n",
    "\n",
    "# TRAINING SET SITE B ------------------------------------------------------------------\n",
    "X_te_list = []\n",
    "y_te = []\n",
    "\n",
    "crop_test(i_img)\n",
    "print(\"Testing...\")\n",
    "print(len(X_te_list))\n",
    "print(len(y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping Training and Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[ 36.  32.  29.  26.  59.  37.  26. 213.  41.  62.] 0\n",
      "(10,)\n",
      "[ 37.  33.  30.  27.  54.  36.  26. 198.  59.  85.] 2\n"
     ]
    }
   ],
   "source": [
    "# TRAINING SET SITE A ------------------------------------------------------------------\n",
    "X_tr = []\n",
    "X_tr = np.array(X_tr_list).reshape(-1, 10)\n",
    "\n",
    "print(X_tr[0].shape)\n",
    "print(X_tr[0], y_tr[0])\n",
    "\n",
    "# TESTING SET SITE B ------------------------------------------------------------------\n",
    "X_te = []\n",
    "X_te = np.array(X_te_list).reshape(-1, 10)\n",
    "\n",
    "print(X_te[0].shape)\n",
    "print(X_te[0], y_te[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Abe-r\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# define the keras model\n",
    "# The model expects rows of data with 10 variables (the input_dim=10 argument)\n",
    "# The first hidden layer has 12 nodes and uses the relu activation function.\n",
    "# The second hidden layer has 8 nodes and uses the relu activation function.\n",
    "# The output layer has one node and uses the sigmoid activation function.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(9, input_dim=10, activation='relu'))\n",
    "model.add(Dense(7, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Abe-r\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/150\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 2/150\n",
      "200/200 [==============================] - 0s 123us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 3/150\n",
      "200/200 [==============================] - 0s 140us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 4/150\n",
      "200/200 [==============================] - 0s 140us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 5/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 6/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 7/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 8/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 9/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 10/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 11/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 12/150\n",
      "200/200 [==============================] - 0s 145us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 13/150\n",
      "200/200 [==============================] - 0s 125us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 14/150\n",
      "200/200 [==============================] - 0s 155us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 15/150\n",
      "200/200 [==============================] - 0s 160us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 16/150\n",
      "200/200 [==============================] - 0s 145us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 17/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 18/150\n",
      "200/200 [==============================] - 0s 135us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 19/150\n",
      "200/200 [==============================] - 0s 135us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 20/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 21/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 22/150\n",
      "200/200 [==============================] - 0s 140us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 23/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 24/150\n",
      "200/200 [==============================] - 0s 135us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 25/150\n",
      "200/200 [==============================] - 0s 140us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 26/150\n",
      "200/200 [==============================] - 0s 150us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 27/150\n",
      "200/200 [==============================] - 0s 135us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 28/150\n",
      "200/200 [==============================] - 0s 132us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 29/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 30/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 31/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 32/150\n",
      "200/200 [==============================] - 0s 150us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 33/150\n",
      "200/200 [==============================] - 0s 135us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 34/150\n",
      "200/200 [==============================] - 0s 135us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 35/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 36/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 37/150\n",
      "200/200 [==============================] - 0s 187us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 38/150\n",
      "200/200 [==============================] - 0s 145us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 39/150\n",
      "200/200 [==============================] - 0s 160us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 40/150\n",
      "200/200 [==============================] - 0s 135us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 41/150\n",
      "200/200 [==============================] - ETA: 0s - loss: 17.7299 - acc: 0.10 - 0s 150us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 42/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 43/150\n",
      "200/200 [==============================] - 0s 140us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 44/150\n",
      "200/200 [==============================] - 0s 150us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 45/150\n",
      "200/200 [==============================] - 0s 140us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 46/150\n",
      "200/200 [==============================] - 0s 150us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 47/150\n",
      "200/200 [==============================] - 0s 150us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 48/150\n",
      "200/200 [==============================] - 0s 145us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 49/150\n",
      "200/200 [==============================] - 0s 135us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 50/150\n",
      "200/200 [==============================] - 0s 155us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 51/150\n",
      "200/200 [==============================] - 0s 145us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 52/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 53/150\n",
      "200/200 [==============================] - 0s 155us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 54/150\n",
      "200/200 [==============================] - 0s 135us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 55/150\n",
      "200/200 [==============================] - 0s 140us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 56/150\n",
      "200/200 [==============================] - 0s 160us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 57/150\n",
      "200/200 [==============================] - 0s 155us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 58/150\n",
      "200/200 [==============================] - 0s 160us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 59/150\n",
      "200/200 [==============================] - 0s 150us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 60/150\n",
      "200/200 [==============================] - 0s 157us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 61/150\n",
      "200/200 [==============================] - 0s 145us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 62/150\n",
      "200/200 [==============================] - 0s 150us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 63/150\n",
      "200/200 [==============================] - 0s 145us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 64/150\n",
      "200/200 [==============================] - 0s 145us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 65/150\n",
      "200/200 [==============================] - 0s 140us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 66/150\n",
      "200/200 [==============================] - 0s 140us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 67/150\n",
      "200/200 [==============================] - 0s 125us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 68/150\n",
      "200/200 [==============================] - 0s 140us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 69/150\n",
      "200/200 [==============================] - 0s 135us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 70/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 71/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 72/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 73/150\n",
      "200/200 [==============================] - 0s 125us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 74/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 75/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 76/150\n",
      "200/200 [==============================] - 0s 125us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 77/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 78/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 125us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 79/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 80/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 81/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 82/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 83/150\n",
      "200/200 [==============================] - 0s 105us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 84/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 85/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 86/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 87/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 88/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 89/150\n",
      "200/200 [==============================] - 0s 100us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 90/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 91/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 92/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 93/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 94/150\n",
      "200/200 [==============================] - 0s 105us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 95/150\n",
      "200/200 [==============================] - 0s 125us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 96/150\n",
      "200/200 [==============================] - 0s 125us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 97/150\n",
      "200/200 [==============================] - 0s 117us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 98/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 99/150\n",
      "200/200 [==============================] - 0s 105us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 100/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 101/150\n",
      "200/200 [==============================] - 0s 105us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 102/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 103/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 104/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 105/150\n",
      "200/200 [==============================] - 0s 105us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 106/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 107/150\n",
      "200/200 [==============================] - 0s 105us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 108/150\n",
      "200/200 [==============================] - 0s 299us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 109/150\n",
      "200/200 [==============================] - 0s 100us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 110/150\n",
      "200/200 [==============================] - 0s 125us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 111/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 112/150\n",
      "200/200 [==============================] - 0s 125us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 113/150\n",
      "200/200 [==============================] - 0s 145us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 114/150\n",
      "200/200 [==============================] - 0s 140us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 115/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 116/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 117/150\n",
      "200/200 [==============================] - 0s 105us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 118/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 119/150\n",
      "200/200 [==============================] - 0s 105us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 120/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 121/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 122/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 123/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 124/150\n",
      "200/200 [==============================] - 0s 125us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 125/150\n",
      "200/200 [==============================] - 0s 105us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 126/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 127/150\n",
      "200/200 [==============================] - 0s 130us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 128/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 129/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 130/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 131/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 132/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 133/150\n",
      "200/200 [==============================] - 0s 140us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 134/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 135/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 136/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 137/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 138/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 139/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 140/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 141/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 142/150\n",
      "200/200 [==============================] - 0s 125us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 143/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 144/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 145/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 146/150\n",
      "200/200 [==============================] - 0s 110us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 147/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 148/150\n",
      "200/200 [==============================] - 0s 182us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 149/150\n",
      "200/200 [==============================] - 0s 120us/sample - loss: 12.8139 - acc: 0.3050\n",
      "Epoch 150/150\n",
      "200/200 [==============================] - 0s 115us/sample - loss: 12.8139 - acc: 0.3050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1206bcdac88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X_tr, y_tr, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 265us/sample - loss: 8.7844 - acc: 0.5250\n",
      "Accuracy: 52.50\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "loss, accuracy = model.evaluate(X_te, y_te)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
